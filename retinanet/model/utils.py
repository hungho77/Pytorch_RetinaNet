from typing import *

# ----------------------------------------------------------------------------- #
# Config definition
# ----------------------------------------------------------------------------- #


# ----------------------------------------------------------------------------- #
# INPUT
# ----------------------------------------------------------------------------- #
# `Mean values` used for input normalization.
MEAN: List[float] = [0.485, 0.456, 0.406]
# `STD values` used for input normalization.
STD: List[float] = [0.229, 0.224, 0.225]
# Size of the smallest side of the image during training
MIN_IMAGE_SIZE: int = 800
# Maximum size of the side of the image during training
MAX_IMAGE_SIZE: int = 1333
# Neck FPN configs
PYRAMID_LEVELS = [3, 4, 5, 6, 7]

# ----------------------------------------------------------------------------- #
# # Anchor generator options
# ----------------------------------------------------------------------------- #
# Anchor sizes (i.e. sqrt of area) in absolute pixels w.r.t. the network input.
# For each area given in `SIZES`, anchors with different aspect

utils_sizes = [2 ** (x + 2) for x in PYRAMID_LEVELS]
utils_scales = [2 ** 0, 2 ** (1.0 / 3.0), 2 ** (2.0 / 3.0)]

ANCHOR_SIZES: List[float] = [[size * utils_scales[0], size * utils_scales[1], size * utils_scales[2]] for size in utils_sizes]

# A list of float value representing the strides for each feature
# map in the feature pyramid.
ANCHOR_STRIDES: List[int] = [2 ** x for x in PYRAMID_LEVELS]

# Anchor aspect ratios. For each area given in `SIZES`, anchors with different aspect
# ratios are generated by an anchor generator.
ANCHOR_ASPECT_RATIOS: List[float] = [0.5, 1.0, 2.0]

# Relative offset between the center of the first anchor and the top-left corner of the image
# Value has to be in [0, 1). Recommend to use 0.5, which means half stride.
# The value is not expected to affect model accuracy.
ANCHOR_OFFSET: float = 0.0


# ----------------------------------------------------------------------------- #
# RetinaNet Head
# ----------------------------------------------------------------------------- #
NUM_CLASSES: int = 90
# This is the number of foreground classes.

# The network used to compute the features for the model.
# Should be one of ['resnet18', 'resnet34', 'resnet50', 'resnet101', 'resnet101', 'resnet152'].
BACKBONE: str = "resnet50"
# Wether the backbone should be pretrained or not,. If true loads `pre-trained` weights
PRETRAINED_BACKBONE: bool = True

# Prior prob for rare case (i.e. foreground) at the beginning of training.
# This is used to set the bias for the logits layer of the classifier subnet.
# This improves training stability in the case of heavy class imbalance.
PRIOR: float = 0.01

# Wether to freeze `BatchNormalization` layers of `backbone`
# If batch size is small set this to True
FREEZE_BN: bool = True

# Weights on (dx, dy, dw, dh) for normalizing Retinanet anchor regression targets
BBOX_REG_WEIGHTS = [1.0, 1.0, 1.0, 1.0]

# Inference cls score threshold, only anchors with score > INFERENCE_TH are
# considered for inference.
SCORE_THRES: float = 0.05
NMS_THRES: float = 0.5
# Maximum number of detections to return per image during inference (100 is
# based on the limit established for the COCO dataset).
MAX_DETECTIONS_PER_IMAGE: int = 100

# IoU overlap ratio bg & fg for labeling anchors.
# Anchors with < bg are labeled negative (0)
# Anchors  with >= bg and < fg are ignored (-1)
# Anchors with >= fg are labeled positive (1)
IOU_THRESHOLDS_FOREGROUND: float = 0.5
IOU_THRESHOLDS_BACKGROUND: float = 0.4

# Loss parameters
FOCAL_LOSS_GAMMA: float = 2.0
FOCAL_LOSS_ALPHA: float = 0.25
SMOOTH_L1_LOSS_BETA: float = 0.1


def ifnone(a: Any, b: Any) -> Any:
    """`a` if `a` is not None, otherwise `b`"""
    if a is not None:
        return a
    else:
        return b